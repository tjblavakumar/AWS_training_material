{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48191036-4ebd-4259-86e9-0144363d1cee",
   "metadata": {},
   "source": [
    "# Video Analysis with Amazon Nova Models\n",
    "\n",
    "In this notebook, we demonstrate how to use Amazon Nova models for a video understanding application. \n",
    "\n",
    "## Use Case Description\n",
    "\n",
    "We will use Nova to analyze security footage for an AnyCompany Telecom retail store. We will use Amazon Nova to directly analyze video for high level description of the footage. Then, we will complete frame by frame analyis for a more detailed task. In this module you will complete the following exercises:\n",
    "\n",
    "\n",
    "- **Summarizing Video:** Extracting a brief summary and description of the provided video\n",
    "- **Frame by frame analysis:** Using image frames extracted from the video to complete a task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb287a1b-0020-4ea4-af35-cfad70572c2b",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29da533e-f623-4fb8-973d-606c931f9152",
   "metadata": {},
   "source": [
    "This module will use ffmpeg, which is an opensource framework for video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c603596-2b7c-4819-bec1-af5092f06768",
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt-get update\n",
    "#ffmpeg install\n",
    "!sudo apt-get -q install ffmpeg -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66242e7-7433-4d1b-b779-3e0ec0d9759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f7d97f-689e-4f8d-bc98-bcd81f790038",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# AWS/Sagemaker imports\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import shutil\n",
    "\n",
    "# Core functionality\n",
    "import os\n",
    "import json\n",
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "# Video and image processing\n",
    "from PIL import Image\n",
    "from IPython.display import Video, Markdown, display, HTML, Image as IPyImage\n",
    "from lib.frames import VideoProcessor\n",
    "\n",
    "# Logging (if needed)\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ec430-65c0-47bc-b84f-14f53e87e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the region from the SageMaker session\n",
    "region_name = sagemaker.Session().boto_region_name\n",
    "print(f\"Current AWS Region: {region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d5e90-0ddd-4e12-a63d-63b320ce1f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "MICRO_MODEL_ID = \"us.amazon.nova-micro-v1:0\"\n",
    "LITE_MODEL_ID = \"us.amazon.nova-lite-v1:0\"\n",
    "PRO_MODEL_ID = \"us.amazon.nova-pro-v1:0\"\n",
    "PREMIER_MODEL_ID = \"us.amazon.nova-premier-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ba989d-23af-48a2-8c46-8c07124678ed",
   "metadata": {},
   "source": [
    "## 1. Summarizing Video: Extracting a brief summary and description of the provided video\n",
    "In this use case we will use Amazon Nova to analyze video using the Invoke API. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af5a3bb-6e71-4c5a-8224-bc87ebf58d0b",
   "metadata": {},
   "source": [
    "Amazon Nova models can process video content in two ways:\n",
    "1. **Base64 Method**: Include encoded video directly in the payload (limited to 25MB total payload size)\n",
    "2. **S3 URI Method**: Reference larger videos (up to 1GB) stored in S3 buckets\n",
    "\n",
    "We'll use the base64 method here. We define a function to call Nova to analyze the video and function to encode as input in the payload to Nova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27748d73-01c5-415c-9ec6-603664bd45fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_nova(\n",
    "    model,\n",
    "    messages,\n",
    "    system_message=\"\",\n",
    "    streaming=False,\n",
    "    max_tokens=3000,\n",
    "    temp=0.1,\n",
    "    top_p=0.99,\n",
    "    top_k=20,\n",
    "    tools=None,\n",
    "    verbose=False,\n",
    "):\n",
    "    \"\"\"Call Amazon Nova models with various parameters.\n",
    "    \n",
    "    Args:\n",
    "        model (str): The model ID to use\n",
    "        messages (list): List of message objects with role and content\n",
    "        system_message (str, optional): System prompt. Defaults to \"\".\n",
    "        streaming (bool, optional): Whether to use streaming API. Defaults to False.\n",
    "        max_tokens (int, optional): Maximum tokens to generate. Defaults to 512.\n",
    "        temp (float, optional): Temperature parameter. Defaults to 0.7.\n",
    "        top_p (float, optional): Top-p parameter. Defaults to 0.99.\n",
    "        top_k (int, optional): Top-k parameter. Defaults to 20.\n",
    "        tools (list, optional): List of tool specifications. Defaults to None.\n",
    "        verbose (bool, optional): Whether to print request body. Defaults to False.\n",
    "        \n",
    "    Returns:\n",
    "        tuple or stream: Model response and content text if not streaming, else stream\n",
    "    \"\"\"\n",
    "    client = boto3.client(\"bedrock-runtime\")\n",
    "    \n",
    "    # Prepare system prompt\n",
    "    system_list = [{\"text\": system_message}]\n",
    "    \n",
    "    # Prepare inference parameters\n",
    "    inf_params = {\n",
    "        \"max_new_tokens\": max_tokens,\n",
    "        \"top_p\": top_p,\n",
    "        \"top_k\": top_k,\n",
    "        \"temperature\": temp,\n",
    "    }\n",
    "    \n",
    "    # Build request body\n",
    "    request_body = {\n",
    "        \"messages\": messages,\n",
    "        \"system\": system_list,\n",
    "        \"inferenceConfig\": inf_params,\n",
    "    }\n",
    "    \n",
    "    # Add tool configuration if provided\n",
    "    if tools is not None:\n",
    "        tool_config = []\n",
    "        for tool in tools:\n",
    "            tool_config.append({\"toolSpec\": tool})\n",
    "        request_body[\"toolConfig\"] = {\"tools\": tool_config}\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Request Body\", request_body)\n",
    "    \n",
    "    if not streaming:\n",
    "        # Use synchronous API\n",
    "        response = client.invoke_model(modelId=model, body=json.dumps(request_body))\n",
    "        model_response = json.loads(response[\"body\"].read())\n",
    "        return model_response, model_response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "    else:\n",
    "        # Use streaming API\n",
    "        response = client.invoke_model_with_response_stream(\n",
    "            modelId=model, body=json.dumps(request_body)\n",
    "        )\n",
    "        return response[\"body\"]\n",
    "\n",
    "\n",
    "def get_base64_encoded_value(media_path):\n",
    "    \"\"\"Convert media file to base64 encoded string.\n",
    "    \n",
    "    Args:\n",
    "        media_path (str): Path to the media file\n",
    "        \n",
    "    Returns:\n",
    "        str: Base64 encoded string\n",
    "    \"\"\"\n",
    "    with open(media_path, \"rb\") as media_file:\n",
    "        binary_data = media_file.read()\n",
    "        base_64_encoded_data = base64.b64encode(binary_data)\n",
    "        base64_string = base_64_encoded_data.decode(\"utf-8\")\n",
    "        return base64_string\n",
    "\n",
    "def print_output(content_text):\n",
    "    \"\"\"Display model output as Markdown.\n",
    "    \n",
    "    Args:\n",
    "        content_text (str): Text to display\n",
    "    \"\"\"\n",
    "    display(Markdown(content_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822bbfd8-8e87-427f-9b82-dbf0ceea2ba5",
   "metadata": {},
   "source": [
    "Let's take a look at the video we are working with. It displays a telecom retail store with employees dressed in pink."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e61a338-87ea-4e06-9236-2cdb463086b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#display the video\n",
    "\n",
    "video_path = \"video/store-footage-01.mp4\"\n",
    "\n",
    "# Verify file exists\n",
    "if os.path.exists(video_path):\n",
    "    # Display video with controls and specified dimensions\n",
    "    display(Video(video_path, \n",
    "                 embed=True, \n",
    "                 width=800,  # Adjust width as needed\n",
    "                 height=450, \n",
    "                 html_attributes=\"controls autoplay loop\"))\n",
    "else:\n",
    "    print(f\"Error: Video file not found at {video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b35bf3-ed54-403e-8c4f-1d6d544ff94d",
   "metadata": {},
   "source": [
    "We'll use a helper function in lib/frames.py to get some technical stats on the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b719779-2723-4e69-87d6-2ff24c4d258c",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = VideoProcessor(\"video/store-footage-01.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5bde8-a828-4421-ba8d-ab360ec7b2fd",
   "metadata": {},
   "source": [
    " The video is captured at 1080p resolution and 24 frames per second."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eef93c-2101-4519-914b-e8208cd855dc",
   "metadata": {},
   "source": [
    "### Video Summarization\n",
    "\n",
    "First we will use Amazon Nova Pro to give a high level summary of the video. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a01c28f-843b-4bb4-bb83-fb98db63eb69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "system_message = f\"\"\"You are an AI assistant specialized in visual analysis and multimodal understanding.\n",
    "Your task is to analyze a video showing a retail store environment. \"\"\"\n",
    "\n",
    "prompt = \"Summarize the events in this video\"\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"video\": {\n",
    "                    \"format\": \"mp4\",\n",
    "                    \"source\": {\n",
    "                        \"bytes\": get_base64_encoded_value(\n",
    "                            \"video/store-footage-01.mp4\"\n",
    "                        )\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"text\":prompt\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "model_response, content_text = call_nova(\n",
    "    PRO_MODEL_ID, messages, system_message=system_message, max_tokens=300\n",
    ")\n",
    "\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print_output(content_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf8470c-4a0a-4abc-9a01-d2525c87a480",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = f\"\"\"You are an AI assistant specialized in visual analysis and multimodal understanding.\n",
    "Your task is to analyze a video showing a retail store environment. \"\"\"\n",
    "\n",
    "prompt = \"\"\"Describe 5 events  that happen in this video. Do not include timestamps \n",
    "Output your response as a bullet point list\n",
    "\n",
    "Apply these definitions of physical location in store (reference these in your response):\n",
    "1. ENTRANCE ZONE - Glass door entry area at top of image\n",
    "2. CENTER PRODUCT DISPLAY TABLES - Wooden tables in center with smartphones/tablets\n",
    "3. LEFT ACCESSORY WALL - Wall displays on left side of the image/video\n",
    "4. RIGHT ACCESSORY WALL - Wall displays on right side of the image/video\n",
    "5. BACK PRODUCT DISPLAY TABLE - Wooden table closet to the camera with devices \n",
    "6. SERVICE COUNTER LEFT - Staffed counter on left\n",
    "7. SERVICE COUNTER RIGHT - Staffed counter on right\n",
    "8. CENTER AISLE - Main walkway through center of the store\n",
    "\n",
    "DO NOT make up events you don't see.\"\"\"\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"video\": {\n",
    "                    \"format\": \"mp4\",\n",
    "                    \"source\": {\n",
    "                        \"bytes\": get_base64_encoded_value(\n",
    "                            \"video/store-footage-01.mp4\"\n",
    "                        )\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"text\":prompt\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "model_response, content_text = call_nova(\n",
    "    PREMIER_MODEL_ID, messages, system_message=system_message, max_tokens=300\n",
    ")\n",
    "\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print_output(content_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42654ed1-8296-47f5-9a2e-f54e2e071b6e",
   "metadata": {},
   "source": [
    "## 2. Frame by frame analysis\n",
    "\n",
    "Frame-by-frame analysis provides greater precision and control over video understanding, allowing us to identify specific moments, track changes between exact points in time, and correlate events with precise timestampsâ€”capabilities that are essential for detailed behavioral analysis and actionable insights. \n",
    "\n",
    "In this exercise, we're going to use frame by frame analysis to analyze a customer's behavior in the store. To do this you have to pre-process the video as follows:\n",
    "\n",
    "### Steps we'll take for pre-processing:\n",
    "1. **Extract frames:** Extract image frames from the video at 4 frames per second (fps)\n",
    "2. **Analyze frames:** Analyze extracted frames to keep frames with distinct actions\n",
    "3. **Create composite image:** Compose a grid of retained frames remaining frames as a composite image for Nova's analysis\n",
    "\n",
    "We wil use a Python package called [VideoProcessor](./lib/frames.py) to complete the pre-processing steps.\n",
    "\n",
    "**Note:** You do have the option to pass each individual frame to Nova for analysis. However, creating a composite of image frames is a useful approach for cases where you are designing for scale and need to optimize cost further "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00b5c99-92cb-4c94-8ecc-93ae682a996f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract frames at 4 fps\n",
    "frames = processor.extract_frames(fps=4, max_resolution=(1280, 720))\n",
    "\n",
    "# Create the composite of distinct frames\n",
    "composite = processor.create_composite_from_distinct_frames(\n",
    "    frame_paths=frames,\n",
    "    columns=4,\n",
    "    similarity_threshold=0.85\n",
    ")\n",
    "\n",
    "# Save and display the composite\n",
    "composite_path = \"distinct_frames_composite.jpg\"\n",
    "composite.save(composite_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7efa65-7d1d-4b36-8b8d-3abe1881e005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the composite image of distinct frames\n",
    "with Image.open(composite_path) as img:\n",
    "    resized_img = img.resize((900, int(900 * img.size[1] / img.size[0])), Image.LANCZOS)\n",
    "    display(IPyImage(data=resized_img._repr_png_()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ae6509-417d-477b-abc8-56c5d8b48340",
   "metadata": {},
   "source": [
    "#### Now we provide that composite image to Nova for a task to analyse a customer's behavior in the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf678a4b-5148-4399-b663-331ea75b7a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = f\"\"\"You are an AI assistant specialized in visual analysis and multimodal understanding.\n",
    "Your task is to analyze footgae showing AnyCompany Telecom retail store. \"\"\"\n",
    "\n",
    "customer_description=f\"\"\"customer in a white button downshirt who is closest to the center \n",
    "of the image in the first frame (closest to the center of the image)\"\"\"\n",
    "\n",
    "\n",
    "prompt = \"\"\"\n",
    "You are analyzing security camera footage from an AnyCompany Telecom retail store displayed as an image. \n",
    "In the image you are looking at a grid of multiple image frame from the security footage.  \n",
    "Analyze the grids from left to right and top to bottom\n",
    "\n",
    "In this footage, there are two types of individuals:\n",
    "- Employees: Wearing pink/magenta shirts\n",
    "- Customers: Not wearing the uniform; They are wearing regular clothing.\n",
    "\n",
    "IMAGE DETAILS:\n",
    "- 21 sequential image frames arranged in a 4 by 6 grid\n",
    "- Extracted at 4 frames per second\n",
    "- Total time span of the original video: 5 seconds\n",
    "\n",
    "\n",
    "ALWAYS Apply these store to each image frame (reference these in your response):\n",
    "1. ENTRANCE ZONE - Glass door entry area at top of image\n",
    "2. CENTER PRODUCT DISPLAY TABLES - Wooden tables in center with smartphones/tablets\n",
    "3. LEFT ACCESSORY WALL - Wall displays on left side\n",
    "4. RIGHT ACCESSORY WALL - Wall displays on right side\n",
    "5. BACK PRODUCT DISPLAY TABLE - Wooden table closet to the camera with devices \n",
    "6. SERVICE COUNTER LEFT - Staffed counter on left\n",
    "7. SERVICE COUNTER RIGHT - Staffed counter on right\n",
    "8. CENTER AISLE - Main walkway through center of the store\n",
    "\n",
    "CUSTOMER TO ANALYZE:\n",
    "{customer_description}\n",
    "\n",
    "YOUR TASK:\n",
    "Analyze this customer frame-by-frame then provide a HIGH-LEVEL summary answering:What was this customer interested in?\n",
    "\n",
    "Provide your analysis in JSON format with this structure:\n",
    "\n",
    "{\n",
    "  \"journey_path\": {\n",
    "    \"zones_visited_in_order\": [\"zone1\", \"zone2\", \"zone3\"],\n",
    "    \"time_in_each_zone\": \"Describe time spent in each zone\",\n",
    "    \"movement_pattern\": \"Describe their movement style\"\n",
    "  },\n",
    "  \n",
    "  \"interest_analysis\": {\n",
    "    \"primary_interest\": \"What are they most interested in?\",\n",
    "    \"evidence\": \"What behaviors show this?\",\n",
    "    \"engagement_level\": \"High or Medium or Low\",\n",
    "    \"specific_items\": \"Any specific products they examined?\"\n",
    "  },\n",
    "  \n",
    "  \"staff_interaction\": {\n",
    "    \"did_they_interact_with_staff\": \"Yes or No or About to\",\n",
    "    \"which_counter\": \"Left or Right or None\",\n",
    "    \"interaction_type\": \"Describe the interaction or N/A\"\n",
    "  }\n",
    "}\n",
    "\n",
    "At all times, be precise, factual, and justify your classifications based on visual evidence from the video.\n",
    "\n",
    "DO NOT make up events you don't see.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"image\": {\n",
    "                    \"format\": \"jpg\",\n",
    "                    \"source\": {\n",
    "                        \"bytes\": get_base64_encoded_value(\n",
    "                            \"distinct_frames_composite.jpg\"\n",
    "                        )\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                \"text\":prompt\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "model_response, content_text = call_nova(\n",
    "    PREMIER_MODEL_ID, messages, system_message=system_message, max_tokens=300\n",
    ")\n",
    "\n",
    "print(\"\\n[Response Content Text]\")\n",
    "print_output(content_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068301ce-8985-43d5-9ab5-4a4c4ad7d792",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove (\"\"\") below to execute this cell and run the code again if interested\n",
    "\n",
    "\"\"\"\n",
    "#delete ./video/storage-footage-01 folder and distinct_frames_composte.jpg to run the code again\n",
    "folder_path = './video/storage-footage-01'\n",
    "file_path = './distinct_frames_composite.jpg'\n",
    "\n",
    "# Delete folder and contents\n",
    "try:\n",
    "    if os.path.exists(folder_path):\n",
    "        shutil.rmtree(folder_path)\n",
    "        print(f\"Successfully deleted {folder_path} and all its contents\")\n",
    "    else:\n",
    "        print(f\"Folder {folder_path} does not exist\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while deleting folder: {e}\")\n",
    "\n",
    "# Delete file\n",
    "try:\n",
    "    if os.path.exists(file_path):\n",
    "        os.remove(file_path)\n",
    "        print(f\"Successfully deleted {file_path}\")\n",
    "    else:\n",
    "        print(f\"File {file_path} does not exist\")\n",
    "except Exception as e:\n",
    "    print(f\"Error occurred while deleting file: {e}\")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31f9913-b8d2-4e71-9b1f-7fe4044ae630",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we demonstrated Amazon Nova's video analysis capabilities, showcasing both direct video processing and frame-by-frame analysis approaches.\n",
    "\n",
    "## Key Learnings\n",
    "\n",
    "1. **Video Analysis Approaches**\n",
    "   - **Direct Analysis**: Used Nova for holistic video understanding\n",
    "   - **Frame Analysis**: Extracted and analyzed individual frames for detailed insights\n",
    "   - **Composite Analysis**: Combined frames for efficient batch processing\n",
    "\n",
    "2. **Technical Implementation**\n",
    "   - Leveraged a reusable video processing pipeline with FFmpeg\n",
    "   - Implemented frame similarity detection to reduce redundancy\n",
    "   - Created composite visualizations for analysis\n",
    "\n",
    "3. **Business Applications**\n",
    "   - Customer journey tracking in retail environments\n",
    "   - Behavioral analysis and movement patterns\n",
    "   - Decision support for customer service\n",
    "\n",
    "\n",
    "This workshop demonstrated how Nova can be effectively used for detailed video understanding tasks while maintaining processing efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411b31a-9862-4735-ac77-ce2d64641e41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
