{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Creating Multimodal RAG using Amazon Bedrock Knowledge Bases\n",
    "\n",
    "Amazon Bedrock Knowledge Bases leverage Retrieval Augmented Generation (RAG), a technique that harnesses customer data stores to enhance responses generated by foundation models. Knowledge bases allow agents to access existing customer data repositories without extensive administrator overhead. To connect a knowledge base to your data, you specify an S3 bucket as the data source. By employing knowledge bases, applications gain enriched contextual information, streamlining development through a fully-managed RAG solution. This level of abstraction accelerates time-to-market by minimizing the effort of incorporating your data into agent functionality and it optimizes cost by negating the necessity for continuous model retraining to leverage private data.\n",
    "Knowledge Base Preparation\n",
    "\n",
    "## Use Case Description\n",
    "\n",
    "We will imagine that we are a Customer Support Agent for AnyCompany Telecom who helps resolve customer issues. We have an AI assistant that we can use to provide quick resolutions to customer queries, ensuring a strong customer experience. This AI asistant uses RAG to retrieve relevant information to resolve connectivity issues, product questions, and more.\n",
    "\n",
    "For this, we would need a RAG System that has these relevant information about a particular hotel. For this exercise, we will will load some documents which have some information about the hotel. This information will include not just text but also other charts, graph information.\n",
    "\n",
    "\n",
    "![mages/mm-rag.png](../images/mm-rag.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade -r ../requirements.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T19:18:51.085558Z",
     "iopub.status.busy": "2025-11-12T19:18:51.085335Z",
     "iopub.status.idle": "2025-11-12T19:18:52.497025Z",
     "shell.execute_reply": "2025-11-12T19:18:52.496536Z",
     "shell.execute_reply.started": "2025-11-12T19:18:51.085542Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "#Install AWS and API tools\n",
    "import boto3\n",
    "import sagemaker\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# Data handling tools\n",
    "import json\n",
    "import base64\n",
    "\n",
    "# Display and formatting tools\n",
    "from IPython.display import display, Image\n",
    "import pprint\n",
    "\n",
    "# System tools\n",
    "import os\n",
    "\n",
    "# Utilities\n",
    "import random\n",
    "import time\n",
    "from retrying import retry\n",
    "\n",
    "boto3_session = boto3.session.Session()\n",
    "pp = pprint.PrettyPrinter(indent=2)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "50f8139a-03b5-4e73-a142-a1d32c03a23d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-12T19:18:55.860186Z",
     "iopub.status.busy": "2025-11-12T19:18:55.859973Z",
     "iopub.status.idle": "2025-11-12T19:18:55.931532Z",
     "shell.execute_reply": "2025-11-12T19:18:55.931086Z",
     "shell.execute_reply.started": "2025-11-12T19:18:55.860170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current AWS Region: us-west-2\n"
     ]
    }
   ],
   "source": [
    "# Get the region from the SageMaker session\n",
    "region_name = sagemaker.Session().boto_region_name\n",
    "print(f\"Current AWS Region: {region_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bab42f-5080-40f2-96bd-da748a79b4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MICRO_MODEL_ID = \"us.amazon.nova-micro-v1:0\"\n",
    "LITE_MODEL_ID = \"us.amazon.nova-lite-v1:0\"\n",
    "PRO_MODEL_ID = \"us.amazon.nova-pro-v1:0\"\n",
    "PREMIER_MODEL_ID = \"us.amazon.nova-premier-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "account_id = boto3.client(\"sts\").get_caller_identity().get(\"Account\")\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "bucket_name = f\"mmu-workshop-{account_id}\"\n",
    "tmp_bucket_name = f\"mmu-workshop-tmp-{account_id}\"\n",
    "\n",
    "r = s3_client.list_buckets(Prefix=bucket_name)\n",
    "if r[\"Buckets\"][0][\"Name\"].startswith(bucket_name):\n",
    "    bucket_name = r[\"Buckets\"][0][\"Name\"]\n",
    "    s3_client.put_object(Bucket=bucket_name, Key=\"mm-data/\")\n",
    "    print(f\"Successfully created mm-data/ folder in {bucket_name}\")\n",
    "    print(f\"S3 URI for Data Source: s3://{bucket_name}/mm-data/\")\n",
    "\n",
    "r = s3_client.list_buckets(Prefix=tmp_bucket_name)\n",
    "if r[\"Buckets\"][0][\"Name\"].startswith(tmp_bucket_name):\n",
    "    tmp_bucket_name = r[\"Buckets\"][0][\"Name\"]\n",
    "    print(f\"S3 URI for Multimodal Storage: s3://{tmp_bucket_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### 1. Create the KB\n",
    "\n",
    "**Step 1:** Navigate to the [Amazon Bedrock > Knowledge base > Create knowledge](https://console.aws.amazon.com/bedrock/home#/knowledge-bases/create-knowledge-base) base console as shown:\n",
    "\n",
    "![images/kb/kb-home.png](../images/kb/kb-home.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "**Step 2**: Next, lets select Create Knowledge Base > Knowledge Base with vector dtore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7064d78",
   "metadata": {},
   "source": [
    "**Step 3**: Next, lets provide knowledge base details such as KB name, Description, etc.\n",
    "\n",
    "In below illustration, we are giving \n",
    "- **KB Name**: `knowledge-base-customer-support` and feel free to put some relevent description\n",
    "- **IAM Role**: Let KB create an IAM role with all needed permissions.\n",
    "- **DataSource**: Next we choose S3 as our Data source, where we will add some menus to use as RAG data source later in the notebook.\n",
    "\n",
    "![images/kb/kb-setup.png](../images/kb/kb-setup.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "**Step 4**: **Configure Data Source**\n",
    "\n",
    "Now, lets configure the data source (S3, in this case) for this to happen we need to provide details as described below:\n",
    "\n",
    "- **S3 URI**: The S3 URI where our multimodal files of the dataset are located. Use the `s3://mmu-workshop-*****/mm-data` as bucket and sub-folder path. \n",
    "- For **Parsing Strategy**, select **Foundation models as a parser** and then choose **Amazon Nova Lite** as model.\n",
    "\n",
    "This means that Nova Lite will be used to parse the multimodal content and summarize the images before passing it to generator model, where we will be using Amazon Nova.\n",
    "\n",
    "![images/kb/kb_config-1.png](../images/kb/kb-config-1.png)\n",
    "\n",
    "\n",
    "![images/kb/kb_config-2](../images/kb/kb-config-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note: S3 URI for Data Source</b>- ⚠️ For S3 location for Data Source choose the bucket where you will store the multimodal PDF files. If you are running this notebook as part of an AWS event using Workshop Studio, you should see a bucket with similar name mmu-workshop-********  and create a partition in there to separate our data such as `mm-data`\n",
    "So the overall S3 URI becomes like:\n",
    "\n",
    "    \n",
    "  `s3://mmu-workshop-********/mm-data`\n",
    "<br>\n",
    "<b>Note: S3 URI for Multimodal Storage</b>- ⚠️ For S3 location for Multimodal Storage(we will show this below) we will create a separate bucket where the parsed images will be stored. If you are using in workshop you should see a bucket with similar name - mmu-workshop-tmp-********\n",
    "So the overall S3 URI becomes like:\n",
    "\n",
    "    \n",
    "  `s3://mmu-workshop-tmp-********`\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "**Step 5: Select Embedding Model and Configure Vector Store**\n",
    "\n",
    "- Select the embedding model **Amazon Titan Embedding Model v2**, which will be used to create vector representations of the multimodal data. \n",
    "- **NOTE:** If you have requested Bedrock model access for only Nova models in the first lab, please make sure to request access for Titan Text Embeddings V2 following the same instructions, [request access following these instructions](https://docs.aws.amazon.com/bedrock/latest/userguide/model-access-modify.html)\n",
    "\n",
    "- For **Vector Database**, select **Open Search Serverless**.\n",
    "\n",
    "- For **Multimodal storage destination**, select the previously discovered S3 bucket for temporary files `mmu-workshop-tmp-******`.\n",
    "\n",
    "![images/kb/storage-config.png](../images/kb/storage-config.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "**Step 6: Review and Create the Knowledge Base**\n",
    "\n",
    "Finally, lets review the details enter, and click submit to create a Multimodal Knowledge base\n",
    "\n",
    "Once the Knowledge base is created (this generally takes 4-5 mins) you would see an image similar to the following message on screen\n",
    "\n",
    "![images/kb/kb_successmsg.png](../images/kb/kb_successmsg.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### This overall process takes roughly 10 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "### 2. Key Information\n",
    "\n",
    "Once the Knowledge Base is created lets note down the following key information in below cell.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Please make sure to add Knowledge Base Name, Id, Data Source ID and the IAM role name created below\n",
    "</div>\n",
    "\n",
    "![images/kb/kb-id.png](../images/kb/kb-id.png)\n",
    "\n",
    "![images/kb/data-source-id.png](../images/kb/data-source-id.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "## ⚠️ ⚠️ replace below values with the created Knowledge Base and Data Source\n",
    "kb_name = \"\" # add your knowledge base name here\n",
    "kb_id = \"\"   # add your knowledge base id here\n",
    "ds_id = \"\"  # add your data source id here\n",
    "kb_iam_role_name = \"\"    #add your knowledge base iam role name here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Verify that all resources are created correctly and ready to be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8de8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_agent_client = boto3_session.client(\"bedrock-agent\")\n",
    "bedrock_agent_runtime_client = boto3.client(\"bedrock-agent-runtime\")\n",
    "\n",
    "get_kb_response = bedrock_agent_client.get_knowledge_base(knowledgeBaseId=kb_id)\n",
    "\n",
    "get_ds_response = bedrock_agent_client.get_data_source(\n",
    "    knowledgeBaseId=kb_id, dataSourceId=ds_id\n",
    ")\n",
    "\n",
    "%store kb_name\n",
    "%store kb_id\n",
    "%store ds_id\n",
    "%store account_id\n",
    "%store bucket_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "### 3. Ingest the customer support information files into the S3 location "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975ee5f5",
   "metadata": {},
   "source": [
    "### Using multiple S3 buckets \n",
    "\n",
    "1. **Data Source Input S3 Bucket**: This s3 bucket will serve as an input for creating our Data Source which will create a Vector Database using OpenSearch Serverless. For this we will use the pre-created bucket of the format\n",
    "`mmu-workshop-<ACCOUNT_ID>-*****`\n",
    "\n",
    "2. **Multimodal Storage Bucket**: This s3 bucket will be used to write and read any extracted images from multimodal documents that needs to be refrenced while answering questions related to images. For this we will use a pre-created bucket of the format\n",
    "`mmu-workshop-tmp-<ACCOUNT_ID>-*****`\n",
    "\n",
    "**Note**: We will be syncing in \"mm-data\" partition, if you are syncing somewhere else please make the appropriate modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f200d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload data to S3 to the bucket that was configured as a data source to the Knowledge Base\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "\n",
    "def interactive_sleep(seconds: int):\n",
    "    dots = \"\"\n",
    "    for i in range(seconds):\n",
    "        dots += \".\"\n",
    "        print(dots, end=\"\\r\")\n",
    "        time.sleep(1)\n",
    "\n",
    "\n",
    "def uploadDirectory(path, bucket_name, s3_path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            local_file_path = os.path.join(root, file)\n",
    "            s3_key = os.path.join(s3_path, os.path.relpath(local_file_path, path))\n",
    "            # Upload the file with the new S3 key\n",
    "            s3_client.upload_file(local_file_path, bucket_name, s3_key)\n",
    "\n",
    "\n",
    "uploadDirectory(\"mm-rag-docs\", bucket_name, \"mm-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c97aaf",
   "metadata": {},
   "source": [
    "### 4. Sync the KB Data Source _[You can also skip this to do from UI by clicking \"Sync Data Source\"]_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ba414f",
   "metadata": {},
   "source": [
    "![images/kb/kb_sync.png](../images/kb/kb_sync.png)\n",
    "\n",
    "This may take 5-7 mins to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactive_sleep(30)\n",
    "ingest_jobs = []\n",
    "\n",
    "# Start an ingestion job\n",
    "try:\n",
    "    start_job_response = bedrock_agent_client.start_ingestion_job(\n",
    "        knowledgeBaseId=kb_id, dataSourceId=ds_id\n",
    "    )\n",
    "    job = start_job_response[\"ingestionJob\"]\n",
    "    job_id = job[\"ingestionJobId\"]\n",
    "    print(f\"Ingestion job started successfully. {job_id=}\")\n",
    "\n",
    "    while job[\"status\"] not in [\"COMPLETE\", \"FAILED\", \"STOPPED\"]:\n",
    "        get_job_response = bedrock_agent_client.get_ingestion_job(\n",
    "            knowledgeBaseId=kb_id, dataSourceId=ds_id, ingestionJobId=job_id\n",
    "        )\n",
    "        job = get_job_response[\"ingestionJob\"]\n",
    "    pp.pprint(job)\n",
    "    interactive_sleep(40)\n",
    "    ingest_jobs.append(job)\n",
    "except Exception as e:\n",
    "    print(\"Failed to start ingestion job!\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "### 5. Update the IAM Policy to include Amazon Nova as generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_policy_json_to_role(role_name, policy_name, policy_json):\n",
    "    \"\"\"\n",
    "    Attaches a policy JSON directly to an IAM role.\n",
    "\n",
    "    :param role_name: The name of the IAM role\n",
    "    :param policy_name: The name to give the new policy\n",
    "    :param policy_json: The policy document as a JSON string or dictionary\n",
    "    :return: True if successful, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create an IAM client\n",
    "        iam_client = boto3.client(\"iam\")\n",
    "\n",
    "        # Ensure policy_json is a string\n",
    "        if isinstance(policy_json, dict):\n",
    "            policy_json = json.dumps(policy_json)\n",
    "\n",
    "        # Create the policy\n",
    "        response = iam_client.create_policy(\n",
    "            PolicyName=policy_name, PolicyDocument=policy_json\n",
    "        )\n",
    "\n",
    "        # Get the ARN of the newly created policy\n",
    "        policy_arn = response[\"Policy\"][\"Arn\"]\n",
    "\n",
    "        # Attach the policy to the role\n",
    "        iam_client.attach_role_policy(RoleName=role_name, PolicyArn=policy_arn)\n",
    "\n",
    "        print(\n",
    "            f\"Successfully created policy {policy_name} and attached it to role {role_name}\"\n",
    "        )\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error attaching policy JSON to role: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Example usage\n",
    "policy_name = \"NovaProModelPolicy\"\n",
    "policy_json = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "            \"Sid\": \"BedrockInvokeModelStatement\",\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Action\": [\"bedrock:*\"],\n",
    "            \"Resource\": [\n",
    "                f\"arn:aws:bedrock:us-east-1::foundation-model/{PRO_MODEL_ID.removeprefix('us.')}\",\n",
    "                f\"arn:aws:bedrock:us-west-2::foundation-model/{PRO_MODEL_ID.removeprefix('us.')}\",\n",
    "                f\"arn:aws:bedrock:us-west-2:{account_id}:inference-profile/{PRO_MODEL_ID}\",\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    "}\n",
    "\n",
    "attach_policy_json_to_role(kb_iam_role_name, policy_name, policy_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "### 6. Test the KB retrieve and retrieve and generate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "### Using RetrieveAndGenerate API\n",
    "Behind the scenes, RetrieveAndGenerate API converts queries into embeddings, searches the knowledge base, and then augments the foundation model prompt with the search results as context information and returns the FM-generated response to the question. For multi-turn conversations, Knowledge Bases manage short-term memory of the conversation to provide more contextual results.\n",
    "\n",
    "The output of the RetrieveAndGenerate API includes the generated response, source attribution as well as the retrieved text chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import s3fs\n",
    "import ipywidgets as widgets\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "\n",
    "def ask_bedrock_llm_with_knowledge_base(query: str, model_arn: str, kb_id: str) -> str:\n",
    "    response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "        input={\"text\": query},\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            \"type\": \"KNOWLEDGE_BASE\",\n",
    "            \"knowledgeBaseConfiguration\": {\n",
    "                \"knowledgeBaseId\": kb_id,\n",
    "                \"modelArn\": model_arn,\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    return response\n",
    "\n",
    "\n",
    "fs = s3fs.S3FileSystem()\n",
    "\n",
    "\n",
    "## Function to print retrieved response\n",
    "def print_response(response):\n",
    "    # structure 'retrievalResults': list of contents. Each list has ['ResponseMetadata', 'citations', 'output', 'sessionId']\n",
    "    generated_text = response[\"output\"][\"text\"]\n",
    "    ref_ref_location_lst = []\n",
    "    ref_ref_location_lst.append({\"generated_text\": generated_text})\n",
    "    for num, chunk in enumerate(response[\"citations\"]):\n",
    "        ref_locations = []\n",
    "        for i, ref in enumerate(chunk[\"retrievedReferences\"]):\n",
    "            data_dict = {\n",
    "                \"ref_location\": ref[\"location\"],\n",
    "                \"ref_metadata\": ref[\"metadata\"][\"x-amz-bedrock-kb-source-uri\"],\n",
    "            }\n",
    "            if \"x-amz-bedrock-kb-byte-content-source\" in ref[\"metadata\"].keys():\n",
    "                data_dict[\"ref_image\"] = ref[\"metadata\"][\n",
    "                    \"x-amz-bedrock-kb-byte-content-source\"\n",
    "                ]\n",
    "            ref_locations.append(data_dict)\n",
    "        ref_ref_location_lst.append({\"chunk_details\": ref_locations})\n",
    "    return ref_ref_location_lst\n",
    "\n",
    "\n",
    "def create_tree_widget(data, s3=None):\n",
    "    s3 = s3 or s3fs.S3FileSystem(anon=False)\n",
    "    main_accordion = widgets.Accordion()\n",
    "\n",
    "    for i, item in enumerate(data):\n",
    "        subchildren = []\n",
    "\n",
    "        # Always add Generated Text first\n",
    "        if \"generated_text\" in item:\n",
    "            text_widget = widgets.Textarea(\n",
    "                value=str(item[\"generated_text\"]),\n",
    "                disabled=True,\n",
    "                layout=widgets.Layout(width=\"500px\", height=\"200px\"),\n",
    "            )\n",
    "            subchildren.append(text_widget)\n",
    "\n",
    "        # Then add Chunk Details\n",
    "        if \"chunk_details\" in item:\n",
    "            chunk_accordion = widgets.Accordion()\n",
    "            chunk_children = []\n",
    "\n",
    "            for chunk in item[\"chunk_details\"]:\n",
    "                chunk_subchildren = []\n",
    "\n",
    "                for key, value in chunk.items():\n",
    "                    if (\n",
    "                        key == \"ref_image\"\n",
    "                        and isinstance(value, str)\n",
    "                        and value.startswith(\"s3://\")\n",
    "                    ):\n",
    "                        try:\n",
    "                            with s3.open(value, \"rb\") as f:\n",
    "                                img = PILImage.open(f).resize((400, 400))\n",
    "                                img_byte_arr = io.BytesIO()\n",
    "                                img.save(img_byte_arr, format=\"PNG\")\n",
    "                                img_widget = widgets.Image(\n",
    "                                    value=img_byte_arr.getvalue(),\n",
    "                                    format=\"png\",\n",
    "                                    width=400,\n",
    "                                    height=400,\n",
    "                                )\n",
    "                            chunk_subchildren.append(img_widget)\n",
    "                        except Exception as e:\n",
    "                            chunk_subchildren.append(widgets.Label(f\"Image Error: {e}\"))\n",
    "                    else:\n",
    "                        chunk_subchildren.append(\n",
    "                            widgets.Label(f\"{key}: {json.dumps(value)}\")\n",
    "                        )\n",
    "\n",
    "                chunk_item_accordion = widgets.Accordion(\n",
    "                    children=tuple(chunk_subchildren)\n",
    "                )\n",
    "                for k, child in enumerate(chunk_subchildren):\n",
    "                    chunk_item_accordion.set_title(k, list(chunk.keys())[k])\n",
    "\n",
    "                chunk_children.append(chunk_item_accordion)\n",
    "\n",
    "            chunk_accordion = widgets.Accordion(children=tuple(chunk_children))\n",
    "            for j, child in enumerate(chunk_children):\n",
    "                chunk_accordion.set_title(j, f\"Chunk {j+1}\")\n",
    "\n",
    "            subchildren.append(chunk_accordion)\n",
    "\n",
    "        # Create item accordion with correct titles\n",
    "        item_accordion = widgets.Accordion(children=tuple(subchildren))\n",
    "        item_accordion.set_title(0, \"Generated Text\")\n",
    "        if len(subchildren) > 1:\n",
    "            item_accordion.set_title(1, \"Chunk Details\")\n",
    "\n",
    "        main_accordion.children += (item_accordion,)\n",
    "        main_accordion.set_title(i, f\"Item {i}\")\n",
    "\n",
    "    return main_accordion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "### Using Textual Information Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"My phone shows -82 dBm signal strength. Is this good enough for streaming video?\"\n",
    "model_arn = f\"arn:aws:bedrock:{region_name}:{account_id}:inference-profile/{PRO_MODEL_ID}\"\n",
    "\n",
    "response = ask_bedrock_llm_with_knowledge_base(query, model_arn, kb_id)\n",
    "response_data = print_response(response)\n",
    "tree_widget = create_tree_widget(response_data)\n",
    "display(tree_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c352cb20-3237-4dcf-a1c2-8bbc19a834f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"I get full bars in my living room but can't make calls. In the bedroom I get one bar but calls work fine. \n",
    "Why is this happening?\"\"\"\n",
    "model_arn = f\"arn:aws:bedrock:{region_name}:{account_id}:inference-profile/{PRO_MODEL_ID}\"\n",
    "\n",
    "response = ask_bedrock_llm_with_knowledge_base(query, model_arn, kb_id)\n",
    "response_data = print_response(response)\n",
    "tree_widget = create_tree_widget(response_data)\n",
    "display(tree_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "### Using Multimodal search to find information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What are good places in my house to put my router?\"\n",
    "\n",
    "response = ask_bedrock_llm_with_knowledge_base(query, model_arn, kb_id)\n",
    "response_data = print_response(response)\n",
    "tree_widget = create_tree_widget(response_data)\n",
    "display(tree_widget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b825ad8b-4938-4bde-bca2-ee6b1fce4a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"My installer mounted my router high up on the wall - seems weird. Is this actually better?\"\n",
    "\n",
    "response = ask_bedrock_llm_with_knowledge_base(query, model_arn, kb_id)\n",
    "response_data = print_response(response)\n",
    "tree_widget = create_tree_widget(response_data)\n",
    "display(tree_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031bb6f4-31d6-4b55-aa08-4055912dbcf3",
   "metadata": {},
   "source": [
    "Now let's test a case that includes an image with the customer's query. For this, we adjust our previous function to include image in the input payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97befcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define function to encode images\n",
    "def get_base64_encoded_value(media_path):\n",
    "    \"\"\"Convert media file to base64 encoded string.\n",
    "    \n",
    "    Args:\n",
    "        media_path (str): Path to the media file\n",
    "        \n",
    "    Returns:\n",
    "        str: Base64 encoded string\n",
    "    \"\"\"\n",
    "    with open(media_path, \"rb\") as media_file:\n",
    "        binary_data = media_file.read()\n",
    "        base_64_encoded_data = base64.b64encode(binary_data)\n",
    "        base64_string = base_64_encoded_data.decode(\"utf-8\")\n",
    "        return base64_string\n",
    "\n",
    "#define function to call Bedrock including an image in the input\n",
    "def ask_bedrock_llm_with_knowledge_base_mm(query: str, image_path: str, model_arn: str, kb_id: str) -> str:\n",
    "    response = bedrock_agent_runtime_client.retrieve_and_generate(\n",
    "        input={\"text\": query},\n",
    "        retrieveAndGenerateConfiguration={\n",
    "            \"type\": \"KNOWLEDGE_BASE\",\n",
    "            \"knowledgeBaseConfiguration\": {\n",
    "                \"knowledgeBaseId\": kb_id,\n",
    "                \"modelArn\": model_arn,\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"image\": {\n",
    "                        \"format\": \"png\",\n",
    "                        \"source\": {\"bytes\": get_base64_encoded_value(image_path)}\n",
    "                    }\n",
    "                },\n",
    "                {\"text\": query}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db91143e-eda9-45d2-abcf-faed29f96264",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"This is the best I can do for my router setup given my skills. Can this work?\"\n",
    "\n",
    "# Path to the damage photo\n",
    "image_path = \"images/wifi-router.png\"\n",
    "\n",
    "# Display the image\n",
    "print(\"Router setup\")\n",
    "display(Image(filename=image_path, width=600))  # Adjust width as needed for clear visibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ca4c7-f6fb-4a76-9938-a7e118c6d4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ask_bedrock_llm_with_knowledge_base_mm(query, image_path, model_arn, kb_id)\n",
    "response_data = print_response(response)\n",
    "tree_widget = create_tree_widget(response_data)\n",
    "display(tree_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458d1d7e",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this lab, we explored how to build a multimodal Retrieval Augmented Generation (RAG) system using Amazon Bedrock Knowledge Bases. We created a customer support assistant that can:\n",
    "\n",
    "1. **Process multimodal documents** - Our system ingested documents containing not just text, but also tables and images\n",
    "2. **Leverage Amazon Nova models** - We used Nova Lite as a parser to extract and understand visual content and Nova Pro as the generator model\n",
    "3. **Build a complete multimodal RAG pipeline** - From creating the knowledge base to configuring the vector store to testing with different query types\n",
    "\n",
    "These capabilities are essential for creating AI assistants that can truly understand and reason about the rich multimodal world of business information."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
